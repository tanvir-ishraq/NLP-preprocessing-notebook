{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# #the preprocessing pipeline process serially:\r\n",
    "# #take data into lists\r\n",
    "\r\n",
    "# #data cleaning:\r\n",
    "# convert to lower case\r\n",
    "# remove stop words\r\n",
    "\r\n",
    "# #preprocessing start:\r\n",
    "# stemming\r\n",
    "# tokenization\r\n",
    "# vectorization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "import json\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "#take data into lists\r\n",
    "\r\n",
    "with open(\"intents.json\", 'r') as f:\r\n",
    "    datastore = json.load(f)\r\n",
    "\r\n",
    "tags = []\r\n",
    "patterns = []\r\n",
    "responses = []\r\n",
    "unique_tags = []  #categorical data numeric form e ana\r\n",
    "\r\n",
    "for item in datastore: #each {} in json file is an item. {} reprensts dictionary in python\r\n",
    "    for sentence in item['patterns'] :\r\n",
    "        patterns.append(sentence)\r\n",
    "        tags.append(item['tag'])\r\n",
    "    for ans in item['responses'] :    \r\n",
    "        responses.append(ans)\r\n",
    "\r\n",
    "# print(patterns)\r\n",
    "# print(tags)\r\n",
    "# print(responses)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# unique category to number make dictionary like food classify\r\n",
    "# #categorical data numeric form e ana\r\n",
    "\r\n",
    "for item in datastore: #each {} in json file is an item\r\n",
    "    unique_tags.append(item['tag'])\r\n",
    "##print(unique_tags)\r\n",
    "\r\n",
    "labels = []\r\n",
    "for i in range(len(unique_tags)):\r\n",
    "    labels.append(i)\r\n",
    "#print(labels)\r\n",
    "\r\n",
    "unique_tags_dictionary = dict(zip(labels, unique_tags))\r\n",
    "print(unique_tags_dictionary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "#dictionary upor base catergory to number presentation\r\n",
    "tags_category_num = []\r\n",
    "for tag in tags:\r\n",
    "    for keys,values in unique_tags_dictionary.items():\r\n",
    "        if(tag == values):\r\n",
    "            tags_category_num.append(keys)\r\n",
    "#print(tags_category_num)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "#for data cleaning:\r\n",
    "import re\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "\r\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\R\n",
      "[nltk_data]     ONE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#stop words remove. then stemmer. then tokenize\r\n",
    "\r\n",
    "#print(patterns) #list of sentence = ['Hi', 'Hey', 'How are you', 'Is anyone there?', 'Hello', 'good morning'\r\n",
    "\r\n",
    "# tf supports basic string normalization (lowercasing + punctuation stripping) \r\n",
    "# no support for  removing stop words\r\n",
    "# just do the standardization beforehand\r\n",
    "def parse_text(text):\r\n",
    "    #print(f'Input: {text}')\r\n",
    "\r\n",
    "    text = re.sub(\"[^a-zA-Z]\", ' ', text) #re.sub(pattern, replacer, main string)\r\n",
    "    #print(f'Remove punctuation and numbers: {text}')\r\n",
    "\r\n",
    "    text = text.lower().split() # = ['is', 'anyone', 'there?']\r\n",
    "    #print(f'Lowercase and split: {text}')\r\n",
    "\r\n",
    "    swords = set(stopwords.words(\"english\"))\r\n",
    "    text = [w for w in text if w not in swords]\r\n",
    "    #print(f'Remove stop words: {text}')\r\n",
    "\r\n",
    "    text = \" \".join(text)\r\n",
    "    #print(f'Final: {text}')\r\n",
    "\r\n",
    "    return text\r\n",
    "\r\n",
    "#stemmer\r\n",
    "from nltk.stem.porter import PorterStemmer\r\n",
    "stemmer = PorterStemmer()\r\n",
    "def stem(text):\r\n",
    "    \"\"\"\r\n",
    "    stemming = find the root form of the word\r\n",
    "    examples:\r\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\r\n",
    "    words = [stem(w) for w in words]\r\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\r\n",
    "    \"\"\"\r\n",
    "    return stemmer.stem(text)\r\n",
    "\r\n",
    "\r\n",
    "patterns_list = []\r\n",
    "for sentence in patterns:\r\n",
    "  patterns_list.append([sentence])\r\n",
    "  \r\n",
    "#data cleaning\r\n",
    "for sublist in patterns_list: # sublist = ['Thanks a lot!']  \r\n",
    "    for i in range(len(sublist)): #sublist[i] = Thanks a lot! #all sublist has one element the string. len(sublist)=1 for all\r\n",
    "        sublist[i] = parse_text(sublist[i])\r\n",
    "\r\n",
    "#print(patterns_list)\r\n",
    "\r\n",
    "#stemming\r\n",
    "for sublist in patterns_list: # sublist = ['Thanks a lot!']  \r\n",
    "    for i in range(len(sublist)): #sublist[i] = Thanks a lot! #all sublist has one element the string. len(sublist)=1 for all\r\n",
    "        sublist[i] = stem(sublist[i])\r\n",
    "\r\n",
    "\r\n",
    "#'list of lists' theke 'list of string' e ana for vectorization.\r\n",
    "patterns_new_list = []\r\n",
    "for sublist in patterns_list: # sublist = ['Thanks a lot!']  \r\n",
    "    for i in range(len(sublist)): #sublist[i] = 'Thanks a lot!' #all sublist has one element the string. len(sublist)=1 for all\r\n",
    "        patterns_new_list.append(sublist[i])\r\n",
    "print(patterns_new_list)\r\n",
    "#print(patterns_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#choosing 20% to be test data.\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(patterns_new_list, tags_category_num, test_size= 0.2)\r\n",
    "\r\n",
    "print(training_sentences)\r\n",
    "print(training_labels)\r\n",
    "# print(testing_sentences)\r\n",
    "# print(testing_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "#vectorization\r\n",
    "# alt is sklearn's CountVectorizer(ngram)\r\n",
    "vocab_size = 10000\r\n",
    "oov_tok = \"<OOV>\" #<OOV> is out of vocab token\r\n",
    "\r\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\r\n",
    "tokenizer.fit_on_texts(training_sentences)\r\n",
    "#bug here\r\n",
    "\r\n",
    "word_index = tokenizer.word_index\r\n",
    "print(word_index)\r\n",
    "\r\n",
    "max_length = 20 #our vector size is not too big\r\n",
    "trunc_type='post'\r\n",
    "padding_type='post'\r\n",
    "#print(training_sequences)\r\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\r\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n",
    "\r\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\r\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<OOV>': 1, 'office': 2, 'offic': 3, 'attendance': 4, 'alchemy': 5, 'attend': 6, 'app': 7, 'provide': 8, 'outside': 9, 'mobile': 10, 'giving': 11, 'fe': 12, 'allow': 13, 'give': 14, 'icms': 15, 'exit': 16, 'day': 17, 'offered': 18, 'outdoor': 19, 'holiday': 20, 'take': 21, 'procedure': 22, 'alchemi': 23, 'use': 24, 'provided': 25, 'rule': 26, 'leav': 27, 'icm': 28, 'leave': 29, 'rul': 30, 'properti': 31, 'tim': 32, 'know': 33, 'govt': 34, 'dress': 35, 'rules': 36, 'put': 37, 'offer': 38, 'working': 39, 'polici': 40, 'month': 41, 'travel': 42, 'capture': 43, 'violation': 44, 'transportation': 45, 'privacy': 46, 'tour': 47, 'allowance': 48, 'work': 49, 'go': 50, 'phon': 51, 'server': 52, 'problem': 53, 'maximum': 54, 'suggestion': 55, 'violate': 56, 'bill': 57, 'time': 58, 'bil': 59, 'maintain': 60, 'cod': 61, 'communication': 62, 'mann': 63, 'indoor': 64, 'week': 65, 'photo': 66, 'hour': 67, 'daily': 68, 'log': 69, 'like': 70, 'viol': 71, 'types': 72, 'tell': 73, 'follow': 74, 'mandatory': 75, 'tea': 76, 'government': 77, 'inconveni': 78, 'procedur': 79, 'without': 80, 'update': 81, 'outsid': 82, 'per': 83, 'code': 84, 'mobil': 85, 'facilities': 86, 'good': 87, 'snack': 88, 'number': 89, 'l': 90, 'snacks': 91, 'maintenance': 92, 'vehicle': 93, 'issu': 94, 'jok': 95, 'monthly': 96, 'privaci': 97, 'sim': 98, 'internet': 99, 'personal': 100, 'us': 101, 'using': 102, 'going': 103, 'late': 104, 'inform': 105, 'input': 106, 'train': 107, 'employee': 108, 'training': 109, 'allowances': 110, 'image': 111, 'shoefie': 112, 'reason': 113, 'must': 114, 'consequ': 115, 'information': 116, 'leaves': 117, 'thank': 118, 'insert': 119, 'get': 120, 'emergency': 121, 'days': 122, 'hiding': 123, 'official': 124, 'breakfast': 125, 'devic': 126, 'appl': 127, 'advance': 128, 'legal': 129, 'others': 130, 'salari': 131, 'coffee': 132, 'mandatori': 133, 'selfie': 134, 'compatible': 135, 'morn': 136, 'goodby': 137, 'imag': 138, 'regular': 139, 'weekly': 140, 'apply': 141, 'advanced': 142, 'coffe': 143, 'starting': 144, 'deserve': 145, 'intim': 146, 'funni': 147, 'phone': 148, 'data': 149, 'policy': 150, 'weekends': 151, 'form': 152, 'application': 153, 'process': 154, 'support': 155, 'weekend': 156, 'regarding': 157, 'client': 158, 'off': 159, 'see': 160, 'lat': 161, 'break': 162, 'penalti': 163, 'hospit': 164, 'info': 165, 'inside': 166, 'protection': 167, 'come': 168, 'wear': 169, 'identity': 170, 'hello': 171, 'updating': 172, 'period': 173, 'sudden': 174, 'outtim': 175, 'hey': 176, 'crim': 177, 'system': 178, 'calculate': 179, 'holidays': 180, 'anyon': 181, 'inquiry': 182, 'capturing': 183, 'suggest': 184, 'smart': 185, 'help': 186, 'trained': 187, 'ident': 188, 'schedul': 189, 'adv': 190, 'picture': 191, 'fee': 192, 'manners': 193, 'hi': 194, 'transport': 195, 'courtesi': 196, 'special': 197, 'present': 198, 'device': 199, 'getting': 200, 'recovery': 201, 'option': 202, 'someth': 203, 'regulations': 204, 'taking': 205}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x NN\r\n",
    "import numpy as np\r\n",
    "training_padded = np.array(training_padded)\r\n",
    "training_labels = np.array(training_labels)\r\n",
    "testing_padded = np.array(testing_padded)\r\n",
    "testing_labels = np.array(testing_labels)\r\n",
    "\r\n",
    "print(training_padded)\r\n",
    "# print(training_labels)\r\n",
    "# print(training_padded.shape)\r\n",
    "# print(training_labels.shape)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('preprocessing': conda)"
  },
  "interpreter": {
   "hash": "5478d44eabea6365c9804b1221ce202976f8cf5d898268ac6315cafab489504e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}